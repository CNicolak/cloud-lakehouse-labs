{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09877c8e-3bc6-4399-9a10-b084854192e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Building a Spark Data pipeline with Delta Lake\n",
    "\n",
    "With this notebook we are buidling an end-to-end pipeline consuming our customers information.\n",
    "\n",
    "We are implementing a *medaillon / multi-hop* architecture, but we could also build a star schema, a data vault or follow any other modeling approach.\n",
    "\n",
    "\n",
    "With traditional systems this can be challenging due to:\n",
    " * data quality issues\n",
    " * running concurrent operations\n",
    " * running DELETE/UPDATE/MERGE operations on files\n",
    " * governance & schema evolution\n",
    " * poor performance from ingesting millions of small files on cloud blob storage\n",
    " * processing & analysing unstructured data (image, video...)\n",
    " * switching between batch or streaming depending of your requirements...\n",
    "\n",
    "## Overcoming these challenges with Delta Lake\n",
    "\n",
    "<div style=\"float:left\">\n",
    "\n",
    "**What's Delta Lake? It's a OSS standard that brings SQL Transactional database capabilities on top of parquet files!**\n",
    "\n",
    "Used as a Spark format, built on top of Spark API / SQL\n",
    "\n",
    "* **ACID transactions** (Multiple writers can simultaneously modify a data set)\n",
    "* **Full DML support** (UPDATE/DELETE/MERGE)\n",
    "* **BATCH and STREAMING** support\n",
    "* **Data quality** (Expectations, Schema Enforcement, Inference and Evolution)\n",
    "* **TIME TRAVEL** (Look back on how data looked like in the past)\n",
    "* **Performance boost** with Z-Order, data skipping and Caching, which solve the small files problem \n",
    "</div>\n",
    "\n",
    "\n",
    "<img src=\"https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-logo.png\" style=\"height: 200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76332e6c-2ec3-4584-92e8-ed5b39132684",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Exploring the dataset\n",
    "\n",
    "Let's review first the raw data landed on our blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9988637-8374-4316-9b18-bb2002157a55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./includes/SetupLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a3e4d5-e98d-48ba-a0bb-8a9849ab28ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "userRawDataDirectory = rawDataDirectory + '/users'\n",
    "print('User raw data under folder: ' + userRawDataDirectory)\n",
    "\n",
    "# Listing the files under the directory\n",
    "for fileInfo in dbutils.fs.ls(userRawDataDirectory): print(fileInfo.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db2007a2-1c31-49b1-a624-ae796acc8f90",
     "showTitle": true,
     "title": "Achieve the same with a \"unix-like\" command"
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /cloud_lakehouse_labs/retail/raw/users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85952eab-5879-4d52-b392-0fcaa24ea3fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Review the raw user data received as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "227bd90a-7838-4b48-8c92-49567fa94518",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM json.`/cloud_lakehouse_labs/retail/raw/users`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb7acf73-88ed-43f9-a69f-34ed040e0fdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Exercise: Try to explore the orders and events data under the /orders and /events subfolders respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d683df-ead5-4914-ae89-6ccbc90bc123",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1/ Loading our data using Databricks Autoloader (cloud_files)\n",
    "<div style=\"float:right\">\n",
    "  <img width=\"700px\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-delta-1.png\"/>\n",
    "</div>\n",
    "  \n",
    "The Autoloader allows us to efficiently ingest millions of files from a cloud storage, and support efficient schema inference and evolution at scale.\n",
    "\n",
    "Let's use it to ingest the raw JSON & CSV data being delivered in our blob storage\n",
    "into the *bronze* tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dbe30e8-ae2f-4db3-8081-02b268621eae",
     "showTitle": true,
     "title": "Storing the raw data in \"bronze\" Delta tables, supporting schema evolution and incorrect data"
    }
   },
   "outputs": [],
   "source": [
    "def ingest_folder(folder, data_format, table):\n",
    "  bronze_products = (spark.readStream\n",
    "                      .format(\"cloudFiles\")\n",
    "                      .option(\"cloudFiles.format\", data_format)\n",
    "                      .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                      .option(\"cloudFiles.schemaLocation\",\n",
    "                              f\"{deltaTablesDirectory}/schema/{table}\") #Autoloader will automatically infer all the schema & evolution\n",
    "                      .load(folder))\n",
    "  return (bronze_products.writeStream\n",
    "            .option(\"checkpointLocation\",\n",
    "                    f\"{deltaTablesDirectory}/checkpoint/{table}\") #exactly once delivery on Delta tables over restart/kill\n",
    "            .option(\"mergeSchema\", \"true\") #merge any new column dynamically\n",
    "            .trigger(once = True) #Remove for real time streaming\n",
    "            .table(table)) #Table will be created if we haven't specified the schema first\n",
    "  \n",
    "ingest_folder(rawDataDirectory + '/orders', 'json', 'churn_orders_bronze')\n",
    "ingest_folder(rawDataDirectory + '/events', 'csv', 'churn_app_events')\n",
    "ingest_folder(rawDataDirectory + '/users', 'json',  'churn_users_bronze').awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d216646d-6c75-4332-9a00-0bdaa29b5a52",
     "showTitle": true,
     "title": "Our user_bronze Delta table is now ready for efficient querying"
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "-- Note the \"_rescued_data\" column. If we receive wrong data not matching existing schema, it will be stored here\n",
    "select * from churn_users_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce4cac2b-e206-4c9f-a5db-d7aa86a54e33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ![](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) 2/ Silver data: anonimized table, date cleaned\n",
    "\n",
    "<img width=\"700px\" style=\"float:right\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-delta-2.png\"/>\n",
    "\n",
    "We can chain these incremental transformation between tables, consuming only new data.\n",
    "\n",
    "This can be triggered in near realtime, or in batch fashion, for example as a job running every night to consume daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c13179f1-ae08-448c-8cfc-62fd898a436c",
     "showTitle": true,
     "title": "Silver table for the users data"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sha1, col, initcap, to_timestamp\n",
    "\n",
    "(spark.readStream\n",
    "        .table(\"churn_users_bronze\")\n",
    "        .withColumnRenamed(\"id\", \"user_id\")\n",
    "        .withColumn(\"email\", sha1(col(\"email\")))\n",
    "        .withColumn(\"creation_date\", to_timestamp(col(\"creation_date\"), \"MM-dd-yyyy H:mm:ss\"))\n",
    "        .withColumn(\"last_activity_date\", to_timestamp(col(\"last_activity_date\"), \"MM-dd-yyyy HH:mm:ss\"))\n",
    "        .withColumn(\"firstname\", initcap(col(\"firstname\")))\n",
    "        .withColumn(\"lastname\", initcap(col(\"lastname\")))\n",
    "        .withColumn(\"age_group\", col(\"age_group\").cast('int'))\n",
    "        .withColumn(\"gender\", col(\"gender\").cast('int'))\n",
    "        .drop(col(\"churn\"))\n",
    "        .drop(col(\"_rescued_data\"))\n",
    "      .writeStream\n",
    "        .option(\"checkpointLocation\", f\"{deltaTablesDirectory}/checkpoint/users\")\n",
    "        .trigger(once=True)\n",
    "        .table(\"churn_users\").awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93fa1529-d8dd-43c3-8643-05c149a29c1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select * from churn_users;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1205eed0-627d-467e-8468-6120cf190a85",
     "showTitle": true,
     "title": "Silver table for the orders data"
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream \n",
    "        .table(\"churn_orders_bronze\")\n",
    "        .withColumnRenamed(\"id\", \"order_id\")\n",
    "        .withColumn(\"amount\", col(\"amount\").cast('int'))\n",
    "        .withColumn(\"item_count\", col(\"item_count\").cast('int'))\n",
    "        .withColumn(\"creation_date\", to_timestamp(col(\"transaction_date\"), \"MM-dd-yyyy H:mm:ss\"))\n",
    "        .drop(col(\"_rescued_data\"))\n",
    "      .writeStream\n",
    "        .option(\"checkpointLocation\", f\"{deltaTablesDirectory}/checkpoint/orders\")\n",
    "        .trigger(once=True)\n",
    "        .table(\"churn_orders\").awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b273db9-dddc-4fad-9951-f978a3d0221c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select * from churn_orders;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a4d5ca-e22e-4343-abc5-8cded5170d53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3/ Aggregate and join data to create our ML features\n",
    "\n",
    "<img width=\"700px\" style=\"float:right\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-delta-3.png\"/>\n",
    "\n",
    "\n",
    "We are now ready to create the features required for our churn prediction.\n",
    "\n",
    "We need to enrich our user dataset with extra information which our model will use to help predicting churn, sucj as:\n",
    "\n",
    "* last command date\n",
    "* number of item bought\n",
    "* number of actions in our website\n",
    "* device used (ios/iphone)\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9b1608-efae-4274-8e21-dee8621be8bc",
     "showTitle": true,
     "title": "Creating a \"gold table\" to be used by the Machine Learning practitioner"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "  \"\"\"\n",
    "    CREATE OR REPLACE TABLE churn_features AS\n",
    "      WITH\n",
    "        churn_orders_stats AS (\n",
    "          SELECT\n",
    "            user_id,\n",
    "            count(*) as order_count,\n",
    "            sum(amount) as total_amount,\n",
    "            sum(item_count) as total_item,\n",
    "            max(creation_date) as last_transaction\n",
    "          FROM churn_orders\n",
    "          GROUP BY user_id\n",
    "        ),  \n",
    "        churn_app_events_stats AS (\n",
    "          SELECT\n",
    "            first(platform) as platform,\n",
    "            user_id,\n",
    "            count(*) as event_count,\n",
    "            count(distinct session_id) as session_count,\n",
    "            max(to_timestamp(date, \"MM-dd-yyyy HH:mm:ss\")) as last_event\n",
    "          FROM churn_app_events GROUP BY user_id\n",
    "        )\n",
    "        SELECT\n",
    "          *, \n",
    "          datediff(now(), creation_date) as days_since_creation,\n",
    "          datediff(now(), last_activity_date) as days_since_last_activity,\n",
    "          datediff(now(), last_event) as days_last_event\n",
    "        FROM churn_users\n",
    "        INNER JOIN churn_orders_stats using (user_id)\n",
    "        INNER JOIN churn_app_events_stats using (user_id)\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "display(spark.table(\"churn_features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d378c3-7b9f-4b75-a70a-1f135990efd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exploiting the benefits of Delta\n",
    "\n",
    "### (a) Simplifing operations with transactional DELETE/UPDATE/MERGE operations\n",
    "\n",
    "Traditional Data Lakes struggle to run even simple DML operations. Using Databricks and Delta Lake, your data is stored on your blob storage with transactional capabilities. You can issue DML operation on Petabyte of data without having to worry about concurrent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9842cb3b-8cb7-4efc-956f-b5bc20994374",
     "showTitle": true,
     "title": "We just realised we have to delete users created before 2016-01-01 for compliance; let's fix that"
    }
   },
   "outputs": [],
   "source": [
    "%sql DELETE FROM churn_users where creation_date < '2016-01-01T03:38:55.000+0000';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950d50ef-3795-4534-9cea-6f79b4681558",
     "showTitle": true,
     "title": "Delta Lake keeps the history of the table operations"
    }
   },
   "outputs": [],
   "source": [
    "%sql describe history churn_users;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6728865-9e4f-473a-a0e0-6526d7b210e9",
     "showTitle": true,
     "title": "We can leverage the history to travel back in time, restore or clone a table, enable CDC, etc."
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    " -- the following also works with AS OF TIMESTAMP \"yyyy-MM-dd HH:mm:ss\"\n",
    "select * from churn_users version as of 1 ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf6fe375-c04d-47c4-ad07-356c72081137",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- You made the DELETE by mistake ? You can easily restore the table at a given version / date:\n",
    "RESTORE TABLE churn_users TO VERSION AS OF 1\n",
    "\n",
    "-- Or clone it (SHALLOW provides zero copy clone):\n",
    "-- CREATE TABLE user_gold_clone SHALLOW|DEEP CLONE user_gold VERSION AS OF 1\n",
    "\n",
    "-- Turn on CDC to capture insert/update/delete operation:\n",
    "-- ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41fd45d5-5d50-436f-b6d0-22bb91f147c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (b) Optimizing for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c477e760-4715-4c3e-8ff3-eb80c10b1286",
     "showTitle": true,
     "title": "Ensuring that all our tables are storage-optimized"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE churn_users    SET TBLPROPERTIES (delta.autooptimize.optimizewrite = TRUE, delta.autooptimize.autocompact = TRUE );\n",
    "ALTER TABLE churn_orders   SET TBLPROPERTIES (delta.autooptimize.optimizewrite = TRUE, delta.autooptimize.autocompact = TRUE );\n",
    "ALTER TABLE churn_features SET TBLPROPERTIES (delta.autooptimize.optimizewrite = TRUE, delta.autooptimize.autocompact = TRUE );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee9a2262-4f08-4ca3-addd-a73360b58d0b",
     "showTitle": true,
     "title": "Our user table will be queried mostly by 3 fields; let's optimize the table for that!"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "OPTIMIZE churn_users ZORDER BY user_id, firstname, lastname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "005fe2f1-7791-4137-99fe-2cfb4cdd75c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Next up\n",
    "* [Exploring, discovering, and governing data access with Unity Catalog]($./01.1 - Unity Catalog)\n",
    "* [Simplifying Data Pipelines with Delta Live Tables]($./01.2 - Delta Live Tables)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3235179284848372,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01 - Data Engineering with Delta",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
