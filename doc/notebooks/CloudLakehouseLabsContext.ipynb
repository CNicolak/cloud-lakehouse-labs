{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c37adedf-43e5-4348-a1cb-8886cfc5a13f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper class that captures the execution context\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "class CloudLakehouseLabsContext:\n",
    "  def __init__(self, useCase: str):\n",
    "    self.__useCase = useCase\n",
    "    self.__cloud = spark.conf.get(\"spark.databricks.clusterUsageTags.cloudProvider\").lower()\n",
    "    self.__user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "    text = self.__user\n",
    "    try: text = unicode(text, 'utf-8')\n",
    "    except (TypeError, NameError): pass\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = text.encode('ascii', 'ignore').decode(\"utf-8\").lower()\n",
    "    self.__user_id = re.sub(\"[^a-zA-Z0-9]\", \"_\", text)\n",
    "\n",
    "    # Create the working schema\n",
    "    catalogName = None\n",
    "    databaseName = self.__user_id + '_' + self.__useCase\n",
    "    for catalog in ['cloud_lakehouse_labs', 'main', 'hive_metastore']:\n",
    "      try:\n",
    "        catalogName = catalog\n",
    "        if catalogName != 'hive_metastore': spark.sql(\"create database if not exists \" + catalog + \".\" + databaseName)\n",
    "        else: spark.sql(\"create database if not exists \" + databaseName)\n",
    "        break\n",
    "      except Exception as e:\n",
    "        pass\n",
    "    if catalogName is None: raise Exception(\"No catalog found with CREATE SCHEMA privileges for user '\" + self.__user + \"'\")\n",
    "    self.__catalog = catalogName\n",
    "    self.__schema = databaseName\n",
    "    if catalogName != 'hive_metastore': spark.sql('use catalog ' + self.__catalog)\n",
    "    spark.sql('use database ' + self.__schema)\n",
    "\n",
    "    # Create the working directory under DBFS\n",
    "    self.__workingDirectory = '/Users/' + self.__user_id + '/' + self.__useCase\n",
    "    dbutils.fs.mkdirs(self.__workingDirectory)\n",
    "\n",
    "  def cloud(self): return self.__cloud\n",
    "\n",
    "  def user(self): return self.__user\n",
    "\n",
    "  def schema(self): return self.__schema\n",
    "\n",
    "  def catalog(self): return self.__catalog\n",
    "\n",
    "  def catalogAndSchema(self): return self.__catalog + '.' + self.__schema\n",
    "\n",
    "  def workingDirectory(self): return self.__workingDirectory\n",
    "\n",
    "  def useCase(self): return self.__useCase\n",
    "\n",
    "  def userId(self): return self.__user_id\n",
    "\n",
    "  def dropAllDataAndSchema(self):\n",
    "    try:\n",
    "      spark.sql('DROP DATABASE IF EXISTS ' + self.catalogAndSchema() + ' CASCADE')\n",
    "    except Exception as e:\n",
    "      print(str(e))\n",
    "    try:\n",
    "      dbutils.fs.rm(self.__workingDirectory, recurse=True)\n",
    "    except Exception as e:\n",
    "      print(str(e))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CloudLakehouseLabsContext",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
