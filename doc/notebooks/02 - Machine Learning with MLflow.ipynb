{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fab9c41-11a4-48c7-9f14-651487103f64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Data Science with Databricks\n",
    "\n",
    "## ML is key to disruption & personalization\n",
    "\n",
    "Being able to ingest and query our C360 database is a first step, but this isn't enough to thrive in a very competitive market.\n",
    "\n",
    "Customers now expect real time personalization and new form of comunication. Modern data company achieve this with AI.\n",
    "\n",
    "<style>\n",
    ".right_box{\n",
    "  margin: 30px; box-shadow: 10px -10px #CCC; width:650px;height:300px; background-color: #1b3139ff; box-shadow:  0 0 10px  rgba(0,0,0,0.6);\n",
    "  border-radius:25px;font-size: 35px; float: left; padding: 20px; color: #f9f7f4; }\n",
    ".badge {\n",
    "  clear: left; float: left; height: 30px; width: 30px;  display: table-cell; vertical-align: middle; border-radius: 50%; background: #fcba33ff; text-align: center; color: white; margin-right: 10px}\n",
    ".badge_b { \n",
    "  height: 35px}\n",
    "</style>\n",
    "<link href='https://fonts.googleapis.com/css?family=DM Sans' rel='stylesheet'>\n",
    "<div style=\"font-family: 'DM Sans'\">\n",
    "  <div style=\"width: 500px; color: #1b3139; margin-left: 50px; float: left\">\n",
    "    <div style=\"color: #ff5f46; font-size:80px\">90%</div>\n",
    "    <div style=\"font-size:30px;  margin-top: -20px; line-height: 30px;\">\n",
    "      Enterprise applications will be AI-augmented by 2025 —IDC\n",
    "    </div>\n",
    "    <div style=\"color: #ff5f46; font-size:80px\">$10T+</div>\n",
    "    <div style=\"font-size:30px;  margin-top: -20px; line-height: 30px;\">\n",
    "       Projected business value creation by AI in 2030 —PWC\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "  <div class=\"right_box\">\n",
    "      But—huge challenges getting ML to work at scale!<br/><br/>\n",
    "      Most ML projects still fail before getting to production\n",
    "  </div>\n",
    "  \n",
    "<br style=\"clear: both\">\n",
    "\n",
    "## Machine learning is data + transforms.\n",
    "\n",
    "ML is hard because delivering value to business lines isn't only about building a Model. <br>\n",
    "The ML lifecycle is made of data pipelines: Data-preprocessing, feature engineering, training, inference, monitoring and retraining...<br>\n",
    "Stepping back, all pipelines are data + code.\n",
    "\n",
    "\n",
    "<img style=\"float: right; margin-top: 10px\" width=\"500px\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/retail/lakehouse-churn/lakehouse-retail-c360-churn-4.png\" />\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/ds.png\" style=\"float: left;\" width=\"80px\"> \n",
    "<h3 style=\"padding: 10px 0px 0px 5px\">Marc, as a Data Scientist, needs a data + ML platform accelerating all the ML & DS steps:</h3>\n",
    "\n",
    "<div style=\"font-size: 19px; margin-left: 73px; clear: left\">\n",
    "<div class=\"badge_b\"><div class=\"badge\">1</div> Build Data Pipeline supporting real time (with DLT)</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">2</div> Data Exploration</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">3</div> Feature creation</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">4</div> Build & train model</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">5</div> Deploy Model (Batch or serverless realtime)</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">6</div> Monitoring</div>\n",
    "</div>\n",
    "\n",
    "**Marc needs A Lakehouse**. Let's see how we can deploy a Churn model in production within the Lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998a0fcb-2531-4a84-a036-df278fb8d4ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###Goal Of Machine Learning vs Traditional Software Development\n",
    "<div><img src=\"https://pages.databricks.com/rs/094-YMS-629/images/mlflowneed.png\" width=\"850\"></div>\n",
    "\n",
    "###MLflow Components\n",
    "\n",
    "<div><img src=\"https://pages.databricks.com/rs/094-YMS-629/images/mlflowcomponents.png\" width=\"850\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe7b8c1e-1f1d-4b00-86b0-f855448382bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Tracking Experiments with MLflow\n",
    "\n",
    "Over the course of the machine learning lifecycle, data scientists test many different models from various libraries with different hyperparemeters.  Tracking these various results poses an organizational challenge.  In brief, storing experiements, results, models, supplementary artifacts, and code creates significant challenges in the machine learning lifecycle.\n",
    "\n",
    "MLflow Tracking is a logging API specific for machine learning and agnostic to libraries and environments that do the training.  It is organized around the concept of **runs**, which are executions of data science code.  Runs are aggregated into **experiments** where many runs can be a part of a given experiment and an MLflow server can host many experiments.\n",
    "\n",
    "Each run can record the following information:\n",
    "\n",
    "- **Parameters:** Key-value pairs of input parameters such as the number of trees in a random forest model\n",
    "- **Metrics:** Evaluation metrics such as RMSE or Area Under the ROC Curve\n",
    "- **Artifacts:** Arbitrary output files in any format.  This can include images, pickled models, and data files\n",
    "- **Source:** The code that originally ran the experiement\n",
    "\n",
    "MLflow tracking also serves as a **model registry** so tracked models can easily be stored and, as necessary, deployed into production.\n",
    "\n",
    "Experiments can be tracked using libraries in Python, R, and Java as well as by using the CLI and REST calls.\n",
    "\n",
    "<div><img src=\"https://pages.databricks.com/rs/094-YMS-629/images/mlflow-tracking.png\" style=\"height: 300px; margin: 20px\"/></div>\n",
    "<div><img src=\"https://pages.databricks.com/rs/094-YMS-629/images/3 - Unify data and ML across the full lifecycle.png\" width=\"950\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38d5a2c7-8170-40e5-b550-432b1e1e0b2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Building a Churn Prediction Model\n",
    "\n",
    "Let's see how we can now leverage the C360 data to build a model predicting and explaining customer Churn.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/retail/lakehouse-churn/lakehouse-retail-churn-ds-flow.png\" width=\"1000px\">\n",
    "\n",
    "*Note: Make sure you switched to the \"Machine Learning\" persona on the top left menu.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b3c7e05-2699-4c1e-8981-00cc51e3d224",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./includes/SetupLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49729bec-d5c0-43f4-bdb3-eb95fbb91ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Our training Data\n",
    "The tables generated with the DLT pipeline contain a **churn** flag which will be used as the label for training of the model.\n",
    "The predictions will eventually be applied to the tables generated with the spark pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34bfecb-82ff-4c0f-af4a-a58d0bc5fb4e",
     "showTitle": true,
     "title": "For the time being the Feature Score is not fully integrated with Unity Catalog"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('USE CATALOG hive_metastore')\n",
    "spark.sql(f'USE {databaseForDLT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7a81b7-8439-4850-bb94-6ced095ba674",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data exploration and analysis\n",
    "\n",
    "Let's review our dataset and start analyze the data we have to predict our churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be1af02f-a6f8-4a39-88a1-1013444a609c",
     "showTitle": true,
     "title": "Read our churn gold table"
    }
   },
   "outputs": [],
   "source": [
    "# Read our churn_features table\n",
    "churn_dataset = spark.table(\"churn_features\")\n",
    "display(churn_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c67517a-f3a8-4269-a32d-cf4a57cccb65",
     "showTitle": true,
     "title": "Data Exploration and analysis"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.PairGrid(churn_dataset.sample(0.01).toPandas()[['age_group','gender','order_count']], diag_sharey=False)\n",
    "g.map_lower(sns.kdeplot)\n",
    "g.map_diag(sns.kdeplot, lw=3)\n",
    "g.map_upper(sns.regplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f4b7947-ec45-416e-a2aa-3435ad239a10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Further data analysis and preparation using pandas API\n",
    "\n",
    "Because our Data Scientist team is familiar with Pandas, we'll use `pandas on spark` to scale `pandas` code. The Pandas instructions will be converted in the spark engine under the hood and distributed at scale.\n",
    "\n",
    "Typicaly a Data Science project would involve more a advanced preparation and likely require extra data prep steps, including more a complex feature preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38a87db3-8767-4f6f-a0d3-01c946383c29",
     "showTitle": true,
     "title": "Custom pandas transformation / code on top of your entire dataset"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to pandas on spark\n",
    "dataset = churn_dataset.pandas_api()\n",
    "dataset.describe()  \n",
    "# Drop columns we don't want to use in our model\n",
    "dataset = dataset.drop(columns=['address', 'email', 'firstname', 'lastname', 'creation_date', 'last_activity_date', 'last_event'])\n",
    "# Drop missing values\n",
    "dataset = dataset.dropna()\n",
    "# print the ten first rows\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a0be95-c1e9-49a8-942c-c96f6824139e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Write to Feature Store\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/mlops-end2end-flow-feature-store.png\" style=\"float:right\" width=\"500\" />\n",
    "\n",
    "Once our features are ready, we can save them in Databricks Feature Store. Under the hood, features store are backed by a Delta Lake table.\n",
    "\n",
    "This will allow discoverability and reusability of our feature across our organization, increasing team efficiency.\n",
    "\n",
    "Feature store will bring traceability and governance in our deployment, knowing which model is dependent of which set of features. It also simplify realtime serving.\n",
    "\n",
    "Make sure you're using the \"Machine Learning\" menu to have access to your feature store using the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ab565ef-027a-4654-9cb1-fb619e1ff46a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "try:\n",
    "  #drop table if exists\n",
    "  fs.drop_table('churn_user_features')\n",
    "except: pass\n",
    "\n",
    "#Note: You might need to delete the FS table using the UI\n",
    "churn_feature_table = fs.create_table(\n",
    "  name='churn_user_features',\n",
    "  primary_keys='user_id',\n",
    "  schema=dataset.spark.schema(),\n",
    "  description='These features are derived from the churn_bronze_customers table in the lakehouse.  We created dummy variables for the categorical columns, cleaned up their names, and added a boolean flag for whether the customer churned or not.  No aggregations were performed.'\n",
    ")\n",
    "\n",
    "fs.write_table(df=dataset.to_spark(), name='churn_user_features', mode='overwrite')\n",
    "features = fs.read_table('churn_user_features')\n",
    "display(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e8ba492-c6f2-4fa3-8cf7-01659e94c857",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Training a model from the table in the Feature Store\n",
    "\n",
    "As we will be using a scikit-learn algorith, we will convert the feature table into a pandas model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ea8237d-cf03-4d8c-a220-29cefd4ebbdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas\n",
    "df = features.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7ed4e4-cfaa-4713-b37c-f2e5f090e623",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Train - test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83225a88-8e02-4a60-a0d0-670177dc57e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split to train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492a40fb-cb00-46cb-8c47-93149dffe484",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define the preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e246497-6c21-40c9-b16c-6a77911b399b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select the columns\n",
    "from databricks.automl_runtime.sklearn.column_selector import ColumnSelector\n",
    "supported_cols = [\"event_count\", \"gender\", \"total_amount\", \"country\", \"order_count\", \"channel\", \"total_item\", \"days_since_last_activity\", \"days_last_event\", \"days_since_creation\", \"session_count\", \"age_group\", \"platform\"]\n",
    "col_selector = ColumnSelector(supported_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e454572-0b94-4936-82f2-d9819cee2639",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "num_imputers = []\n",
    "num_imputers.append((\"impute_mean\", SimpleImputer(), [\"age_group\", \"days_last_event\", \"days_since_creation\", \"days_since_last_activity\", \"event_count\", \"gender\", \"order_count\", \"session_count\", \"total_amount\", \"total_item\"]))\n",
    "\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    (\"converter\", FunctionTransformer(lambda df: df.apply(pd.to_numeric, errors=\"coerce\"))),\n",
    "    (\"imputers\", ColumnTransformer(num_imputers)),\n",
    "    (\"standardizer\", StandardScaler()),\n",
    "])\n",
    "\n",
    "numerical_transformers = [(\"numerical\", numerical_pipeline, [\"event_count\", \"gender\", \"total_amount\", \"order_count\", \"total_item\", \"days_since_last_activity\", \"days_last_event\", \"days_since_creation\", \"session_count\", \"age_group\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0875e9b-0e10-4783-9002-768a3a125d98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Treating categorical variables\n",
    "from databricks.automl_runtime.sklearn import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "one_hot_imputers = []\n",
    "one_hot_pipeline = Pipeline(steps=[\n",
    "    (\"imputers\", ColumnTransformer(one_hot_imputers, remainder=\"passthrough\")),\n",
    "    (\"one_hot_encoder\", OneHotEncoder(handle_unknown=\"indicator\")),\n",
    "])\n",
    "categorical_one_hot_transformers = [(\"onehot\", one_hot_pipeline, [\"age_group\", \"channel\", \"country\", \"event_count\", \"gender\", \"order_count\", \"platform\", \"session_count\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1793d15c-2531-4e26-9b52-faa43aca1b98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final transformation of the columns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "transformers = numerical_transformers + categorical_one_hot_transformers\n",
    "preprocessor = ColumnTransformer(transformers, remainder=\"passthrough\", sparse_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf88075-27a5-472a-970e-39488a3b01e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Separate target column from features\n",
    "target_col = \"churn\"\n",
    "X_train = train_df.drop([target_col], axis=1)\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_test = test_df.drop([target_col], axis=1)\n",
    "y_test = test_df[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f137e593-3819-4454-94b9-c1a57942f276",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Training a model and logging everything with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bfc61ed-6078-4b31-b32d-77d03230f049",
     "showTitle": false,
     "title": "Train a model and log it with MLflow"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.models import Model\n",
    "from mlflow import pyfunc\n",
    "from mlflow.pyfunc import PyFuncModel\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Start a run\n",
    "with mlflow.start_run(run_name=\"simple-RF-run\") as run:\n",
    "  classifier = RandomForestClassifier()\n",
    "  model = Pipeline([\n",
    "      (\"column_selector\", col_selector),\n",
    "      (\"preprocessor\", preprocessor),\n",
    "      (\"classifier\", classifier),\n",
    "  ])\n",
    "\n",
    "  # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "  mlflow.sklearn.autolog(\n",
    "      log_input_examples=True,\n",
    "      silent=True)\n",
    "\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Log metrics for the test set\n",
    "  mlflow_model = Model()\n",
    "  pyfunc.add_to_model(mlflow_model, loader_module=\"mlflow.sklearn\")\n",
    "  pyfunc_model = PyFuncModel(model_meta=mlflow_model, model_impl=model)\n",
    "  X_test[target_col] = y_test\n",
    "  test_eval_result = mlflow.evaluate(\n",
    "      model=pyfunc_model,\n",
    "      data=X_test,\n",
    "      targets=target_col,\n",
    "      model_type=\"classifier\",\n",
    "      evaluator_config = {\"log_model_explainability\": False,\n",
    "                          \"metric_prefix\": \"test_\" , \"pos_label\": 1 }\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6096e1-ea6f-4a35-adab-0c5420b6a33d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Explore the above in the UI\n",
    "\n",
    "From the experiments page select the \"02 - Machine Learning with MLflow\" experiment and see the associated runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb7ffb2-adfe-4de2-86cd-f7770949bd2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"5802ff47-58b5-4789-973d-2fb855bf347a\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Model Registry\n",
    "\n",
    "The MLflow Model Registry component is a centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of an MLflow Model. It provides model lineage (which MLflow Experiment and Run produced the model), model versioning, stage transitions (e.g. from staging to production), annotations (e.g. with comments, tags), and deployment management (e.g. which production jobs have requested a specific model version).\n",
    "\n",
    "Model registry has the following features:<br><br>\n",
    "\n",
    "* **Central Repository:** Register MLflow models with the MLflow Model Registry. A registered model has a unique name, version, stage, and other metadata.\n",
    "* **Model Versioning:** Automatically keep track of versions for registered models when updated.\n",
    "* **Model Stage:** Assigned preset or custom stages to each model version, like “Staging” and “Production” to represent the lifecycle of a model.\n",
    "* **Model Stage Transitions:** Record new registration events or changes as activities that automatically log users, changes, and additional metadata such as comments.\n",
    "* **CI/CD Workflow Integration:** Record stage transitions, request, review and approve changes as part of CI/CD pipelines for better control and governance.\n",
    "\n",
    "<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-4/model-registry.png\" style=\"height: 400px; margin: 20px\"/></div>\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"/> See <a href=\"https://mlflow.org/docs/latest/registry.html\" target=\"_blank\">the MLflow docs</a> for more details on the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "612f789c-5af5-4353-af86-20500924e668",
     "showTitle": true,
     "title": "Register the model in the model registry"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "logged_model = 'runs:/' + run.info.run_id + '/model'\n",
    "\n",
    "print(\"Registeting the model under the name '\" + modelName + \"'\")\n",
    "result=mlflow.register_model(logged_model, modelName, await_registration_for=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf43349-0865-48ef-928e-6166a34135f7",
     "showTitle": true,
     "title": "Retrieving the model status and moving it to the \"Production\" stage"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Retrieving the model\n",
    "client = MlflowClient()\n",
    "model_version_details = None\n",
    "while True:\n",
    "  model_version_details = client.get_model_version(name=modelName, version=result.version)\n",
    "  if model_version_details.status == 'READY': break\n",
    "  time.sleep(5)\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=model_version_details.name,\n",
    "    version=model_version_details.version,\n",
    "    stage=\"Production\",\n",
    "    archive_existing_versions = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "165b6e4b-658a-44b2-96ec-e1282c168dfe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Accelerating Churn model creation using MLFlow and Databricks Auto-ML\n",
    "\n",
    "MLflow is an open source project allowing model tracking, packaging and deployment. Everytime your datascientist team work on a model, Databricks will track all the parameter and data used and will save it. This ensure ML traceability and reproductibility, making it easy to know which model was build using which parameters/data.\n",
    "\n",
    "### A glass-box solution that empowers data teams without taking away control\n",
    "\n",
    "While Databricks simplify model deployment and governance (MLOps) with MLFlow, bootstraping new ML projects can still be long and inefficient. \n",
    "\n",
    "Instead of creating the same boilerplate for each new project, Databricks Auto-ML can automatically generate state of the art models for Classifications, regression, and forecast.\n",
    "\n",
    "\n",
    "<img width=\"1000\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/auto-ml-full.png\"/>\n",
    "\n",
    "\n",
    "Models can be directly deployed, or instead leverage generated notebooks to boostrap projects with best-practices, saving you weeks of efforts.\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "\n",
    "<img style=\"float: right\" width=\"600\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/churn-auto-ml.png\"/>\n",
    "\n",
    "### Using Databricks Auto ML with our Churn dataset\n",
    "\n",
    "Auto ML is available in the \"Machine Learning\" space. All we have to do is start a new Auto-ML experimentation and select the feature table we just created (`churn_features`)\n",
    "\n",
    "Our prediction target is the `churn` column.\n",
    "\n",
    "Click on Start, and Databricks will do the rest.\n",
    "\n",
    "While this is done using the UI, you can also leverage the [python API](https://docs.databricks.com/applications/machine-learning/automl.html#automl-python-api-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e638b76e-ff2b-41d2-8835-3d134b86f6e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Lab exercise - AutoML\n",
    "\n",
    "Let's create a better model with just a few clicks!\n",
    "* Create an AutoML experiment\n",
    "* register the best run with the model named as above.\n",
    "* Promote the new version to **Production**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c60bdc02-56b7-4f9a-bbc9-270c73d74a52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Next up\n",
    "[Use the model to predict the churn]($./02.1 - Machine Learning - Inference)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02 - Machine Learning with MLflow",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
