{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41fc1d74-e420-4f00-9e1e-9c9319f11962",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Churn Prediction Inference - Batch or serverless real-time\n",
    "\n",
    "\n",
    "After running AutoML we saved our best model our MLflow registry.\n",
    "\n",
    "All we need to do now is use this model to run Inferences. A simple solution is to share the model name to our Data Engineering team and they'll be able to call this model within the pipeline they maintained.\n",
    "\n",
    "This can be done as part of a DLT pipeline or a Workflow in a separate job.\n",
    "Here is an example to show you how MLflow can be directly used to retrieve the model and run inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "833de089-5a2d-445d-b63d-16d6d0cf169c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Deploying the model for batch inferences\n",
    "\n",
    "<img style=\"float: right; margin-left: 20px\" width=\"600\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/churn_batch_inference.gif\" />\n",
    "\n",
    "Now that our model is available in the Registry, we can load it to compute our inferences and save them in a table to start building dashboards.\n",
    "\n",
    "We will use MLFlow function to load a pyspark UDF and distribute our inference in the entire cluster. If the data is small, we can also load the model with plain python and use a pandas Dataframe.\n",
    "\n",
    "If you don't know how to start, Databricks can generate a batch inference notebook in just one click from the model registry: Open MLFlow model registry and click the \"User model for inference\" button!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36f4714a-f476-4771-aeed-827714328721",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5/ Enriching the gold data with a ML model\n",
    "<div style=\"float:right\">\n",
    "  <img width=\"500px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-small-4.png\"/>\n",
    "</div>\n",
    "\n",
    "Our Data scientist team has build a churn prediction model using Auto ML and saved it into Databricks Model registry. \n",
    "\n",
    "One of the key value of the Lakehouse is that we can easily load this model and predict our churn right into our pipeline. \n",
    "\n",
    "Note that we don't have to worry about the model framework (sklearn or other), MLflow abstracts all that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f8bd90e-7158-4573-a227-030db51d6f1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./includes/SetupLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8943ef5-46c4-42ee-bb70-5e8ed53fca59",
     "showTitle": true,
     "title": "Loading the model"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "#                                      Stage/version\n",
    "#                       Model name          |              output\n",
    "#                           |               |                 |\n",
    "modelURL = \"models:/\" + modelName + \"/Production\"    #        |\n",
    "print(\"Retrieving model \" + modelURL)                #        |\n",
    "predict_churn_udf = mlflow.pyfunc.spark_udf(spark, modelURL, \"int\")\n",
    "#We can use the function in SQL\n",
    "spark.udf.register(\"predict_churn\", predict_churn_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c99b899-3506-4b74-aee8-e9b35198e050",
     "showTitle": true,
     "title": "Creating the final table"
    }
   },
   "outputs": [],
   "source": [
    "model_features = predict_churn_udf.metadata.get_input_schema().input_names()\n",
    "predictions = spark.table('churn_features').withColumn('churn_prediction', predict_churn_udf(*model_features))\n",
    "predictions.createOrReplaceTempView(\"v_churn_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "442d774e-8103-442a-af57-214dd07abc08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table churn_prediction as select * from v_churn_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de47f623-8f39-4c8f-a093-e27d283c679e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from churn_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6163590-422e-438b-bf06-9c77374d39ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Next up\n",
    "[Explore the data with SQL and create visualisations]($./03 - BI and Data Warehousing)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02.1 - Machine Learning - Inference",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
