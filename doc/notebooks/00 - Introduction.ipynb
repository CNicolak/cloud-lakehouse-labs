{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841d9b23-d051-466e-91a2-7f00d087ac74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Let's start with a business problem:\n",
    "\n",
    "## *Building a Customer 360 database and reducing customer churn with the Databricks Lakehouse*\n",
    "\n",
    "In this demo, we'll step in the shoes of a retail company selling goods with a recurring business.\n",
    "\n",
    "The business has determined that the focus must be placed on churn. We're asked to:\n",
    "\n",
    "* Analyse and explain current customer churn: quantify churn, trends and the impact for the business\n",
    "* Build a proactive system to forecast and reduce churn by taking automated action: targeted email, phoning etc.\n",
    "\n",
    "\n",
    "### What we'll build\n",
    "\n",
    "To do so, we'll build an end-to-end solution with the Lakehouse. To be able to properly analyse and predict our customer churn, we need information coming from different external systems: Customer profiles coming from our website, order details from our ERP system and mobile application clickstream to analyse our customers activity.\n",
    "\n",
    "At a very high level, this is the flow we'll implement:\n",
    "\n",
    "<img width=\"1000px\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/retail/lakehouse-churn/lakehouse-retail-c360-churn-0.png\" />\n",
    "\n",
    "1. Ingest and create our Customer 360 database, with tables easy to query in SQL\n",
    "2. Secure data and grant read access to the Data Analyst and Data Science teams.\n",
    "3. Run BI queries to analyse existing churn\n",
    "4. Build ML model to predict which customer is going to churn and why\n",
    "\n",
    "As a result, we will have all the information required to trigger custom actions to increase retention (email personalized, special offers, phone call...)\n",
    "\n",
    "### Our dataset\n",
    "\n",
    "For simplicity, we will assume that an external system is periodically sending data into our blob cloud storage:\n",
    "\n",
    "- Customer profile data *(name, age, address etc)*\n",
    "- Orders history *(what ours customers have bought over time)*\n",
    "- Events from our application *(when was the last time a customer used the application, what clics were recorded, typically collected through a stream)*\n",
    "\n",
    "*Note that at our data could be arriving from any source. Databricks can ingest data from any system (SalesForce, Fivetran, queuing message like kafka, blob storage, SQL & NoSQL databases...).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28a951d7-b48a-4500-9cb8-a91e4cae499a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Raw data generation\n",
    "\n",
    "For this demonstration we will not be using real data or an existing dataset, but will rather generate them.\n",
    "\n",
    "The cell below will execute a notebook that will generate the data and store them on DBFS.\n",
    "If you want to see the actual code [click here to open it on a different tab]($./includes/CreateRawData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef7a729f-15f0-4ba3-8093-a133c4b49265",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: Faker in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6265a51b-ede1-4a6b-a2b5-75e2ffaddd18/lib/python3.9/site-packages (23.2.1)\nRequirement already satisfied: python-dateutil>=2.4 in /databricks/python3/lib/python3.9/site-packages (from Faker) (2.8.2)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.4->Faker) (1.16.0)\nPython interpreter will be restarted.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data already exists\n"
     ]
    }
   ],
   "source": [
    "%run ./includes/CreateRawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf979cc5-3653-4fa1-931a-91999f3ed50e",
     "showTitle": true,
     "title": "The raw data on DBFS"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order raw data stored under the DBFS folder \"/cloud_lakehouse_labs/retail/raw/orders\"\nUser raw data stored under the DBFS folder \"/cloud_lakehouse_labs/retail/raw/users\"\nWebsite event raw data stored under the DBFS folder \"/cloud_lakehouse_labs/retail/raw/events\"\n"
     ]
    }
   ],
   "source": [
    "ordersFolder = rawDataDirectory + '/orders'\n",
    "usersFolder = rawDataDirectory + '/users'\n",
    "eventsFolder = rawDataDirectory + '/events'\n",
    "print('Order raw data stored under the DBFS folder \"' + ordersFolder + '\"')\n",
    "print('User raw data stored under the DBFS folder \"' + usersFolder + '\"')\n",
    "print('Website event raw data stored under the DBFS folder \"' + eventsFolder + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6141d624-a7b4-4cbf-bc04-6cac073216a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What we are going to implement\n",
    "\n",
    "We will initially load the raw data with the autoloader,\n",
    "perform some cleaning and enrichment operations,\n",
    "develop and load a model from MLFlow to predict our customer churn,\n",
    "and finally use this information to build our DBSQL dashboard to track customer behavior and churn.\n",
    " \n",
    " Bronze layer: Data close to source. Silver: QA. Gold: Highest grain.\n",
    "<div><img width=\"1100px\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-delta.png\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d49fec9e-9c8a-4aee-87a2-84833e0f790e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Let's start with\n",
    "[Data Engineering with Delta]($./01 - Data Engineering with Delta)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00 - Introduction",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
