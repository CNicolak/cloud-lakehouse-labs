{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3bf3616-dab4-483d-9045-e9abd397d989",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Orchestrating our Churn pipeline with Databricks Workflows\n",
    "\n",
    "<img style=\"float: right; margin-left: 10px\" width=\"600px\" src=\"https://www.databricks.com/wp-content/uploads/2022/05/workflows-orchestrate-img.png\" />\n",
    "\n",
    "With Databricks Lakehouse, no need for external orchestrator. We can use [Workflows](/#job/list) (available on the left menu) to orchestrate our Churn pipeline within a few click.\n",
    "\n",
    "\n",
    "\n",
    "###  Orchestrate anything anywhere\n",
    "With workflow, you can run diverse workloads for the full data and AI lifecycle on any cloud. Orchestrate Delta Live Tables and Jobs for SQL, Spark, notebooks, dbt, ML models and more.\n",
    "\n",
    "### Simple - Fully managed\n",
    "Remove operational overhead with a fully managed orchestration service, so you can focus on your workflows not on managing your infrastructure.\n",
    "\n",
    "### Proven reliability\n",
    "Have full confidence in your workflows leveraging our proven experience running tens of millions of production workloads daily across clouds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20b9e1aa-7e74-42a0-b1c7-aa0d05a7415c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating your workflow\n",
    "\n",
    "<img style=\"float: right; margin-left: 10px\" width=\"600px\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-workflow.png\" />\n",
    "\n",
    "A Databricks Workflow is composed of Tasks.\n",
    "\n",
    "Each task can trigger a specific job:\n",
    "\n",
    "* Delta Live Tables\n",
    "* SQL query / dashboard\n",
    "* Model retraining / inference\n",
    "* Notebooks\n",
    "* dbt\n",
    "* ...\n",
    "\n",
    "In this example, can see our 3 tasks:\n",
    "\n",
    "* Start the DLT pipeline to ingest new data and refresh our tables\n",
    "* Refresh the DBSQL dashboard (and potentially notify downstream applications)\n",
    "* Retrain our Churn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "288ed4a8-9d74-4c71-bb9b-dfe1e3d7d732",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Monitoring your runs\n",
    "\n",
    "<img style=\"float: right; margin-left: 10px\" width=\"600px\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-workflow-monitoring.png\" />\n",
    "\n",
    "Once your workflow is created, we can access historical runs and receive alerts if something goes wrong!\n",
    "\n",
    "In the screenshot we can see that our workflow had multiple errors, with different runtime, and ultimately got fixed.\n",
    "\n",
    "Workflow monitoring includes errors, abnormal job duration and more advanced control!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2b3b48e-37a9-4b81-b2ac-68fac7ed48e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Lab exercise - Create a Workflow\n",
    "\n",
    "From the Workflows page **Create a New Job** with the following tasks\n",
    "* **1. Ingest_and_process_new_data**\n",
    "Use the notebook [01 - Data Engineering with Delta]($./01 - Data Engineering with Delta) as the task source\n",
    "* **2. Create_Predictions**\n",
    "Use the notebook [02.1 - Machine Learning - Inference]($./02.1 - Machine Learning - Inference) as the task source<br>\n",
    "*Important:* for this task create a new job cluster that runs on an ML-enabled runtime!\n",
    "* **3. Refresh_Dashboard**\n",
    "Specify **SQL** for the task type and **Dashboard** as the SQL task.\n",
    "Select the dashboard you created in the previous step as well as an existing SQL Warehouse\n",
    "\n",
    "Save and **Run now**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa05c403-d70d-4a3a-8548-0ed6ceb0450b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Congratulations!\n",
    "You have reached the end of this lab and learned how to **create business value** in record time thanks to the **Databricks Lakehouse.**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04 - Orchestrating with Workflows",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
