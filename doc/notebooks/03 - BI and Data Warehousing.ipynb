{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56e69777-c921-4849-9fbd-73eac52a36f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Your Lakehouse is the best Warehouse\n",
    "\n",
    "Traditional Data Warehouses can’t keep up with the variety of data and use cases. Business agility requires reliable, real-time data, with insight from ML models.\n",
    "\n",
    "Working with the lakehouse unlock traditional BI analysis but also real time applications having a direct connection to your entire data, while remaining fully secured.\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/dbsql.png\" width=\"700px\" style=\"float: left\" />\n",
    "<div style=\"float: left; margin-top: 240px; font-size: 23px\">\n",
    "  Instant, elastic compute<br>\n",
    "  Lower TCO with Serveless<br>\n",
    "  Zero management<br>\n",
    "  Governance layer - row level<br>\n",
    "  Your data. Your schema (star, data vault…)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e583e27a-b225-4ab0-9827-a08e3fe04165",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# BI & Datawarehousing with Databricks SQL\n",
    "\n",
    "<img style=\"float: right; margin-top: 10px\" width=\"500px\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/retail/lakehouse-churn/lakehouse-retail-c360-churn-3.png\" />\n",
    "\n",
    "Our datasets are now properly ingested, secured, with a high quality and easily discoverable within our organization.\n",
    "\n",
    "Let's explore how Databricks SQL support your Data Analyst team with interactive BI and start analyzing our customer Churn.\n",
    "\n",
    "To start with Databricks SQL, open the SQL view on the top left menu.\n",
    "\n",
    "You'll be able to:\n",
    "\n",
    "- Create a SQL Warehouse to run your queries\n",
    "- Use DBSQL to build your own dashboards\n",
    "- Plug any BI tools (Tableau/PowerBI/..) to run your analysis\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://www.google-analytics.com/collect?v=1&gtm=GTM-NKQ8TT7&tid=UA-163989034-1&cid=555&aip=1&t=event&ec=field_demos&ea=display&dp=%2F42_field_demos%2Fretail%2Flakehouse_churn%2Fbi&dt=LAKEHOUSE_RETAIL_CHURN\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71365bce-82a9-4ef8-969e-e62e523c7f59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Databricks SQL Warehouses: best-in-class BI engine\n",
    "\n",
    "<img style=\"float: right; margin-left: 10px\" width=\"600px\" src=\"https://www.databricks.com/wp-content/uploads/2022/06/how-does-it-work-image-5.svg\" />\n",
    "\n",
    "Databricks SQL is a warehouse engine packed with thousands of optimizations to provide you with the best performance for all your tools, query types and real-world applications. <a href='https://www.databricks.com/blog/2021/11/02/databricks-sets-official-data-warehousing-performance-record.html'>It holds the Data Warehousing Performance Record.</a>\n",
    "\n",
    "This includes the next-generation vectorized query engine Photon, which together with SQL warehouses, provides up to 12x better price/performance than other cloud data warehouses.\n",
    "\n",
    "**Serverless warehouse** provide instant, elastic SQL compute — decoupled from storage — and will automatically scale to provide unlimited concurrency without disruption, for high concurrency use cases.\n",
    "\n",
    "Make no compromise. Your best Datawarehouse is a Lakehouse.\n",
    "\n",
    "### Creating a SQL Warehouse\n",
    "\n",
    "SQL Wharehouse are managed by databricks. [Creating a warehouse](/sql/warehouses) is a 1-click step: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b93a10e-7134-4baf-a346-ba9d96ce112a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating your first Query\n",
    "\n",
    "<img style=\"float: right; margin-left: 10px\" width=\"600px\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-dbsql-query.png\" />\n",
    "\n",
    "Our users can now start running SQL queries using the SQL editor and add new visualizations.\n",
    "\n",
    "By leveraging auto-completion and the schema browser, we can start running adhoc queries on top of our data.\n",
    "\n",
    "While this is ideal for Data Analyst to start analysing our customer Churn, other personas can also leverage DBSQL to track our data ingestion pipeline, the data quality, model behavior etc.\n",
    "\n",
    "Open the [Queries menu](/sql/queries) to start writting your first analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2756e6da-8f71-4375-9c06-4d42270aa137",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./includes/SetupLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48bc12fa-e5b1-4b02-8bdf-b61073f15eeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following exercise use the following catalog.schema : \ncloud_lakehouse_labs.odl_user_1237583_databrickslabs_com_retail\n"
     ]
    }
   ],
   "source": [
    "print(\"For the following exercise use the following catalog.schema : \\n\" + labContext.catalogAndSchema() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7827f9d2-b300-4239-bdc1-3fa5f54eb7ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Lab exercise\n",
    "\n",
    "Create the following queries and visualisations using the above catalog and schema\n",
    "\n",
    "**1. Total MRR**\n",
    "```\n",
    "SELECT\n",
    "  sum(amount)/1000 as MRR\n",
    "FROM churn_orders\n",
    "WHERE\n",
    "\tmonth(to_timestamp(transaction_date, 'MM-dd-yyyy HH:mm:ss')) = \n",
    "  (\n",
    "    select max(month(to_timestamp(transaction_date, 'MM-dd-yyyy HH:mm:ss')))\n",
    "  \tfrom churn_orders\n",
    "  );\n",
    "```\n",
    "Create a *counter* visualisation\n",
    "\n",
    "**2. MRR at Risk**\n",
    "```\n",
    "SELECT\n",
    "\tsum(amount)/1000 as MRR_at_risk\n",
    "FROM churn_orders\n",
    "WHERE month(to_timestamp(transaction_date, 'MM-dd-yyyy HH:mm:ss')) = \n",
    "\t(\n",
    "\t\tselect max(month(to_timestamp(transaction_date, 'MM-dd-yyyy HH:mm:ss')))\n",
    "\t\tfrom churn_orders\n",
    "\t)\n",
    "\tand user_id in\n",
    "\t(\n",
    "\t\tSELECT user_id FROM churn_prediction WHERE churn_prediction=1\n",
    "\t)\n",
    "```\n",
    "Create a *counter* visualisation\n",
    "\n",
    "**3. Customers at risk**\n",
    "```\n",
    "SELECT count(*) as Customers, cast(churn_prediction as boolean) as `At Risk`\n",
    "FROM churn_prediction GROUP BY churn_prediction;\n",
    "```\n",
    "\n",
    "### For 4 and 5 switch to the schema in the hive metastore where the DLT tables are created \n",
    "\n",
    "**4. Customer Tenure - Historical**\n",
    "```\n",
    "SELECT cast(days_since_creation/30 as int) as days_since_creation, churn, count(*) as customers\n",
    "FROM churn_features\n",
    "GROUP BY days_since_creation, churn having days_since_creation < 1000\n",
    "```\n",
    "Create a *bar* visualisation\n",
    "\n",
    "**5. Subscriptions by Internet Service - Historical**\n",
    "```\n",
    "select platform, churn, count(*) as event_count\n",
    "from churn_app_events\n",
    "inner join churn_users using (user_id)\n",
    "where platform is not null\n",
    "group by platform, churn\n",
    "```\n",
    "Create a *horizontal bar* visualisation\n",
    "\n",
    "### For the rest switch back to the original catalog and schema (if applicable) \n",
    "\n",
    "**6. Predicted to churn by channel**\n",
    "```\n",
    "SELECT channel, count(*) as users\n",
    "FROM churn_prediction\n",
    "WHERE churn_prediction=1 and channel is not null\n",
    "GROUP BY channel\n",
    "```\n",
    "Create a *pie chart* visualisation\n",
    "\n",
    "**7. Predicted to churn by country**\n",
    "```\n",
    "SELECT country, churn_prediction, count(*) as customers\n",
    "FROM churn_prediction\n",
    "GROUP BY country, churn_prediction\n",
    "```\n",
    "Create a *bar* visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bc9f36a-12b3-4d24-8bf7-35ce40f78c05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating our Churn Dashboard\n",
    "\n",
    "<img style=\"float: right; margin-left: 10px\" width=\"600px\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-dbsql-dashboard.png\" />\n",
    "\n",
    "The next step is now to assemble our queries and their visualization in a comprehensive SQL dashboard that our business will be able to track.\n",
    "\n",
    "### Lab exercise\n",
    "Assemple the visualisations defined with the above queries into a dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6d9928b-4017-4396-a624-d2cc3b6573ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f254fcb-8775-47df-9174-6a120b55c491",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Using Third party BI tools\n",
    "\n",
    "<iframe style=\"float: right\" width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/EcKqQV0rCnQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "SQL warehouse can also be used with an external BI tool such as Tableau or PowerBI.\n",
    "\n",
    "This will allow you to run direct queries on top of your table, with a unified security model and Unity Catalog (ex: through SSO). Now analysts can use their favorite tools to discover new business insights on the most complete and freshest data.\n",
    "\n",
    "To start using your Warehouse with third party BI tool, click on \"Partner Connect\" on the bottom left and chose your provider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35bebfaf-129a-479f-9901-30f144ad061c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Going further with DBSQL & Databricks Warehouse\n",
    "\n",
    "Databricks SQL offers much more and provides a full warehouse capabilities\n",
    "\n",
    "<img style=\"float: right\" width=\"400px\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-dbsql-pk-fk.png\" />\n",
    "\n",
    "### Data modeling\n",
    "\n",
    "Comprehensive data modeling. Save your data based on your requirements: Data vault, Star schema, Inmon...\n",
    "\n",
    "Databricks let you create your PK/FK, identity columns (auto-increment)\n",
    "\n",
    "### Data ingestion made easy with DBSQL & DBT\n",
    "\n",
    "Turnkey capabilities allow analysts and analytic engineers to easily ingest data from anything like cloud storage to enterprise applications such as Salesforce, Google Analytics, or Marketo using Fivetran. It’s just one click away. \n",
    "\n",
    "Then, simply manage dependencies and transform data in-place with built-in ETL capabilities on the Lakehouse (Delta Live Table), or using your favorite tools like dbt on Databricks SQL for best-in-class performance.\n",
    "\n",
    "### Query federation\n",
    "\n",
    "Need to access cross-system data? Databricks SQL query federation let you define datasources outside of databricks (ex: PostgreSQL)\n",
    "\n",
    "### Materialized views\n",
    "\n",
    "Avoid expensive queries and materialize your tables. The engine will recompute only what's required when your data get updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4726e3a5-7222-4d94-a2cf-695f8a16f940",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Next up\n",
    "[Orchestrating and automating with Workflows]($./04 - Orchestrating with Workflows)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03 - BI and Data Warehousing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
