{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32495341-7a5d-4ad2-9ad7-bd32f0cc5bb2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Ensuring Governance and security for our C360 lakehouse\n",
    "\n",
    "Data governance and security is hard when it comes to a complete Data Platform. SQL GRANT on tables isn't enough and security must be enforced for multiple data assets (dashboards, Models, files etc).\n",
    "\n",
    "To reduce risks and driving innovation, Emily's team needs to:\n",
    "\n",
    "- Unify all data assets (Tables, Files, ML models, Features, Dashboards, Queries)\n",
    "- Onboard data with multiple teams\n",
    "- Share & monetize assets with external Organizations\n",
    "\n",
    "<style>\n",
    ".box{\n",
    "  box-shadow: 20px -20px #CCC; height:300px; box-shadow:  0 0 10px  rgba(0,0,0,0.3); padding: 5px 10px 0px 10px;}\n",
    ".badge {\n",
    "  clear: left; float: left; height: 30px; width: 30px;  display: table-cell; vertical-align: middle; border-radius: 50%; background: #fcba33ff; text-align: center; color: white; margin-right: 10px}\n",
    ".badge_b { \n",
    "  height: 35px}\n",
    "</style>\n",
    "<link href='https://fonts.googleapis.com/css?family=DM Sans' rel='stylesheet'>\n",
    "<div style=\"padding: 20px; font-family: 'DM Sans'; color: #1b5162\">\n",
    "  <div style=\"width:200px; float: left; text-align: center\">\n",
    "    <div class=\"box\" style=\"\">\n",
    "      <div style=\"font-size: 26px;\">\n",
    "        <strong>Team A</strong>\n",
    "      </div>\n",
    "      <div style=\"font-size: 13px\">\n",
    "        <img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/da.png\" style=\"\" width=\"60px\"> <br/>\n",
    "        Data Analysts<br/>\n",
    "        <img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/ds.png\" style=\"\" width=\"60px\"> <br/>\n",
    "        Data Scientists<br/>\n",
    "        <img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/de.png\" style=\"\" width=\"60px\"> <br/>\n",
    "        Data Engineers\n",
    "      </div>\n",
    "    </div>\n",
    "    <div class=\"box\" style=\"height: 80px; margin: 20px 0px 50px 0px\">\n",
    "      <div style=\"font-size: 26px;\">\n",
    "        <strong>Team B</strong>\n",
    "      </div>\n",
    "      <div style=\"font-size: 13px\">...</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div style=\"float: left; width: 400px; padding: 0px 20px 0px 20px\">\n",
    "    <div style=\"margin: 20px 0px 0px 20px\">Permissions on queries, dashboards</div>\n",
    "    <img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/horizontal-arrow-dash.png\" style=\"width: 400px\">\n",
    "    <div style=\"margin: 20px 0px 0px 20px\">Permissions on tables, columns, rows</div>\n",
    "    <img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/horizontal-arrow-dash.png\" style=\"width: 400px\">\n",
    "    <div style=\"margin: 20px 0px 0px 20px\">Permissions on features, ML models, endpoints, notebooks…</div>\n",
    "    <img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/horizontal-arrow-dash.png\" style=\"width: 400px\">\n",
    "    <div style=\"margin: 20px 0px 0px 20px\">Permissions on files, jobs</div>\n",
    "    <img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/horizontal-arrow-dash.png\" style=\"width: 400px\">\n",
    "  </div>\n",
    "  \n",
    "  <div class=\"box\" style=\"width:550px; float: left\">\n",
    "    <img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/gov.png\" style=\"float: left; margin-right: 10px;\" width=\"80px\"> \n",
    "    <div style=\"float: left; font-size: 26px; margin-top: 0px; line-height: 17px;\"><strong>Emily</strong> <br />Governance and Security</div>\n",
    "    <div style=\"font-size: 18px; clear: left; padding-top: 10px\">\n",
    "      <ul style=\"line-height: 2px;\">\n",
    "        <li>Central catalog - all data assets</li>\n",
    "        <li>Data exploration & discovery to unlock new use-cases</li>\n",
    "        <li>Permissions cross-teams</li>\n",
    "        <li>Reduce risk with audit logs</li>\n",
    "        <li>Measure impact with lineage</li>\n",
    "      </ul>\n",
    "      + Monetize & Share data with external organization (Delta Sharing)\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3c85a3-ccf0-4958-9f42-eccd40054400",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Implementing a global data governance and security with Unity Catalog\n",
    "\n",
    "<img style=\"float: right; margin-top: 30px\" width=\"500px\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/retail/lakehouse-churn/lakehouse-retail-c360-churn-2.png\" />\n",
    "\n",
    "Let's see how the Lakehouse can solve this challenge leveraging Unity Catalog.\n",
    "\n",
    "Our Data has been saved as Delta Table by our Data Engineering team.  The next step is to secure this data while allowing cross team to access it. <br>\n",
    "A typical setup would be the following:\n",
    "\n",
    "* Data Engineers / Jobs can read and update the main data/schemas (ETL part)\n",
    "* Data Scientists can read the final tables and update their features tables\n",
    "* Data Analyst have READ access to the Data Engineering and Feature Tables and can ingest/transform additional data in a separate schema.\n",
    "* Data is masked/anonymized dynamically based on each user access level\n",
    "\n",
    "This is made possible by Unity Catalog. When tables are saved in the Unity Catalog, they can be made accessible to the entire organization, cross-workpsaces and cross users.\n",
    "\n",
    "Unity Catalog is key for data governance, including creating data products or organazing teams around datamesh. It brings among other:\n",
    "\n",
    "* Fined grained ACL\n",
    "* Audit log\n",
    "* Data lineage\n",
    "* Data exploration & discovery\n",
    "* Sharing data with external organization (Delta Sharing)\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://www.google-analytics.com/collect?v=1&gtm=GTM-NKQ8TT7&tid=UA-163989034-1&cid=555&aip=1&t=event&ec=field_demos&ea=display&dp=%2F42_field_demos%2Fretail%2Flakehouse_churn%2Fuc&dt=LAKEHOUSE_RETAIL_CHURN\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27cf2ba6-739a-4512-b3d5-8fc47f8858c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exploring our Customer360 database\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/uc/uc-base-1.png\" style=\"float: right\" width=\"800px\"/> \n",
    "\n",
    "Let's review the data created.\n",
    "\n",
    "Unity Catalog works with 3 layers:\n",
    "\n",
    "* CATALOG\n",
    "* SCHEMA (or DATABASE)\n",
    "* TABLE\n",
    "\n",
    "All unity catalog is available with SQL (`CREATE CATALOG IF NOT EXISTS my_catalog` ...)\n",
    "\n",
    "To access one table, you can specify the full path: `SELECT * FROM &lt;CATALOG&gt;.&lt;SCHEMA&gt;.&lt;TABLE&gt;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4294502c-cf22-4375-8fdd-de2ff1fce0da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Let's review the tables we created under our schema\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-data-explorer.gif\" style=\"float: right\" width=\"800px\"/> \n",
    "\n",
    "Unity Catalog provides a comprehensive Data Explorer that you can access on the left menu.\n",
    "\n",
    "You'll find all your tables, and can use it to access and administrate your tables.\n",
    "\n",
    "They'll be able to create extra table into this schema.\n",
    "\n",
    "### Discoverability \n",
    "\n",
    "In addition, Unity catalog also provides explorability and discoverability. \n",
    "\n",
    "Anyone having access to the tables will be able to search it and analyze its main usage. <br>\n",
    "You can use the Search menu (⌘ + P) to navigate in your data assets (tables, notebooks, queries...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55093c4-30aa-42db-adbd-d1f0878b96e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./includes/SetupLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3304510-3769-460a-9598-2a7c612f54c4",
     "showTitle": true,
     "title": "The above script sets up the default catalog and schema"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working catalog and schema: cloud_lakehouse_labs.odl_user_1237583_databrickslabs_com_retail\n"
     ]
    }
   ],
   "source": [
    "print(\"Working catalog and schema: \" + labContext.catalogAndSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f077e7d5-1385-407e-ac24-5b3ce364f8ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>current_catalog()</th></tr></thead><tbody><tr><td>cloud_lakehouse_labs</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "cloud_lakehouse_labs"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "current_catalog()",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- This lab has been set up to use a specific catalog, if it has been set up, or \"main\". \n",
    "SELECT CURRENT_CATALOG();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b869591e-86ab-4d1c-844c-5a90b5adcaf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>databaseName</th></tr></thead><tbody><tr><td>information_schema</td></tr><tr><td>odl_instructor_1036177_databrickslabs_com_retail</td></tr><tr><td>odl_user_1225167_databrickslabs_com_retail</td></tr><tr><td>odl_user_1237583_databrickslabs_com_retail</td></tr><tr><td>odl_user_1238721_databrickslabs_com_retail</td></tr><tr><td>odl_user_1238938_databrickslabs_com_retail</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "information_schema"
        ],
        [
         "odl_instructor_1036177_databrickslabs_com_retail"
        ],
        [
         "odl_user_1225167_databrickslabs_com_retail"
        ],
        [
         "odl_user_1237583_databrickslabs_com_retail"
        ],
        [
         "odl_user_1238721_databrickslabs_com_retail"
        ],
        [
         "odl_user_1238938_databrickslabs_com_retail"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "databaseName",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Shows the schemas (databases) in the the current catalog\n",
    "SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df45a4e2-802e-4e47-b869-bf35b78238e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>odl_user_1237583_databrickslabs_com_retail</td><td>churn_app_events</td><td>false</td></tr><tr><td>odl_user_1237583_databrickslabs_com_retail</td><td>churn_features</td><td>false</td></tr><tr><td>odl_user_1237583_databrickslabs_com_retail</td><td>churn_orders</td><td>false</td></tr><tr><td>odl_user_1237583_databrickslabs_com_retail</td><td>churn_orders_bronze</td><td>false</td></tr><tr><td>odl_user_1237583_databrickslabs_com_retail</td><td>churn_users</td><td>false</td></tr><tr><td>odl_user_1237583_databrickslabs_com_retail</td><td>churn_users_bronze</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "odl_user_1237583_databrickslabs_com_retail",
         "churn_app_events",
         false
        ],
        [
         "odl_user_1237583_databrickslabs_com_retail",
         "churn_features",
         false
        ],
        [
         "odl_user_1237583_databrickslabs_com_retail",
         "churn_orders",
         false
        ],
        [
         "odl_user_1237583_databrickslabs_com_retail",
         "churn_orders_bronze",
         false
        ],
        [
         "odl_user_1237583_databrickslabs_com_retail",
         "churn_users",
         false
        ],
        [
         "odl_user_1237583_databrickslabs_com_retail",
         "churn_users_bronze",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Shows the tables in the current database\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "795e2fd7-21ec-4206-86b6-ea60f5113f63",
     "showTitle": true,
     "title": "Give access to your schema to other users / or groups"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3235179284848309>:8\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m     display(df)\n",
       "\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m----> 8\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m     10\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-3235179284848309>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m   \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLS0gRklMTCBJTiA8U0NIRU1BPiBhbmQgPFRBQkxFPgpHUkFOVCBVU0UgU0NIRU1BIE9OIFNDSEVNQSA8U0NIRU1BPiBUTyBgYWNjb3VudCB1c2Vyc2A=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m   df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mR1JBTlQgU0VMRUNUIE9OIFRBQkxFIDxTQ0hFTUE+LjxUQUJMRT4gVE8gYGFjY291bnQgdXNlcnNg\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n",
       "\u001B[1;32m      6\u001B[0m   display(df)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mParseException\u001B[0m: \n",
       "Operation not allowed: GRANT.(line 2, pos 0)\n",
       "\n",
       "== SQL ==\n",
       "-- FILL IN <SCHEMA> and <TABLE>\n",
       "GRANT USE SCHEMA ON SCHEMA <SCHEMA> TO `account users`\n",
       "^^^\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-3235179284848309>:8\u001B[0m\n\u001B[1;32m      6\u001B[0m     display(df)\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m----> 8\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     10\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-3235179284848309>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m   \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLS0gRklMTCBJTiA8U0NIRU1BPiBhbmQgPFRBQkxFPgpHUkFOVCBVU0UgU0NIRU1BIE9OIFNDSEVNQSA8U0NIRU1BPiBUTyBgYWNjb3VudCB1c2Vyc2A=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m   df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mR1JBTlQgU0VMRUNUIE9OIFRBQkxFIDxTQ0hFTUE+LjxUQUJMRT4gVE8gYGFjY291bnQgdXNlcnNg\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[1;32m      6\u001B[0m   display(df)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mParseException\u001B[0m: \nOperation not allowed: GRANT.(line 2, pos 0)\n\n== SQL ==\n-- FILL IN <SCHEMA> and <TABLE>\nGRANT USE SCHEMA ON SCHEMA <SCHEMA> TO `account users`\n^^^\n",
       "errorSummary": "<span class='ansi-red-fg'>ParseException</span>: \nOperation not allowed: GRANT.(line 2, pos 0)\n\n== SQL ==\n-- FILL IN <SCHEMA> and <TABLE>\nGRANT USE SCHEMA ON SCHEMA <SCHEMA> TO `account users`\n^^^\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- FILL IN <SCHEMA> and <TABLE>\n",
    "GRANT USE SCHEMA ON SCHEMA <SCHEMA> TO `account users`;\n",
    "GRANT SELECT ON TABLE <SCHEMA>.<TABLE> TO `account users`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a5e0dc-acef-4f4b-9955-7ef8642d7798",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Going further with Data governance & security\n",
    "\n",
    "By bringing all your data assets together, Unity Catalog let you build a complete and simple governance to help you scale your teams.\n",
    "\n",
    "Unity Catalog can be leveraged from simple GRANT to building a complete datamesh organization.\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/uc/lineage/lineage-table.gif\" style=\"float: right; margin-left: 10px\"/>\n",
    "\n",
    "### Fine-grained ACL\n",
    "\n",
    "Need more advanced control? You can chose to dynamically change your table output based on the user permissions.\n",
    "\n",
    "### Secure external location (S3/ADLS/GCS)\n",
    "\n",
    "Unity Catatalog let you secure your managed table but also your external locations.\n",
    "\n",
    "### Lineage \n",
    "\n",
    "UC automatically captures table dependencies and let you track how your data is used, including at a row level.\n",
    "\n",
    "This leat you analyze downstream impact, or monitor sensitive information across the entire organization (GDPR).\n",
    "\n",
    "\n",
    "### Audit log\n",
    "\n",
    "UC captures all events. Need to know who is accessing which data? Query your audit log.\n",
    "\n",
    "This leat you analyze downstream impact, or monitor sensitive information across the entire organization (GDPR).\n",
    "\n",
    "### Upgrading to UC\n",
    "\n",
    "Already using Databricks without UC? Upgrading your tables to benefit from Unity Catalog is simple.\n",
    "\n",
    "### Sharing data with external organization\n",
    "\n",
    "Sharing your data outside of your Databricks users is simple with Delta Sharing, and doesn't require your data consumers to use Databricks."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3235179284848309,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01.1 - Unity Catalog",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
